# Introduction

GPT-2 implementation from scratch \
Basically same specification with GPT-2(base), 124M, but trained with [book corpus](https://huggingface.co/datasets/rojagtap/bookcorpus) instead


